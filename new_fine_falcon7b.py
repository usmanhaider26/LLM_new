# -*- coding: utf-8 -*-
"""Lateste_Falcon7b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Mqp6Z3M7qbxytKCp0hggL1Vg7SNSK4y
"""

# from google.colab import drive
# drive.mount('drive')

# !nvidia-smi

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

# !pip install -q -U bitsandbytes
# !pip install -q -U git+https://github.com/huggingface/transformers.git
# !pip install -q -U git+https://github.com/huggingface/peft.git
# !pip install -q -U git+https://github.com/huggingface/accelerate.git
# !pip install -q -U einops
# !pip install -q -U safetensors
# !pip install -q -U torch
# !pip install -q -U xformers
# !pip install -q -U datasets

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
import transformers
import torch
from torch.utils.data import DataLoader, Dataset
import torch
from transformers import AutoTokenizer
import os

from peft import (
    LoraConfig,
    PeftConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)

# !unzip Generated_data.zip

path = "Generated_data_2x"
dirs = os.listdir( path )

import pandas as pd
import json
data= []
id = 1
for files in dirs:
  f = open(os.path.join(path, files))
  j_data = json.load(f)
  for dd in j_data:
    if all(key in dd for key in ["question", "answer"]):
      temp = {'question': dd['question'],'answer':dd['answer']}
      data.append(temp)
    if all(key in dd for key in ["Question", "Answer"]):
      temp = {'question': dd['Question'],'answer':dd['Answer']}
      data.append(temp)
  # Closing file
  f.close()

df_data = pd.DataFrame(data)

df_data

import re

# Preprocess question column
df_data['question'] = df_data['question'].apply(lambda x: re.sub(r'[^\w\s]', '', x))  # Remove special characters
df_data['question'] = df_data['question'].apply(lambda x: x.lower())  # Convert to lowercase

# Preprocess answer column
df_data['answer'] = df_data['answer'].apply(lambda x: re.sub(r'[^\w\s]', '', x))  # Remove special characters
df_data['answer'] = df_data['answer'].apply(lambda x: x.lower())  # Convert to lowercase

# Display the preprocessed dataframe
print(df_data.head())

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

shuffled_df = shuffle(df_data, random_state=42)

# Split the shuffled DataFrame into train and test sets
data_train, data_test = train_test_split(shuffled_df, test_size=0.15, random_state=42)

# Print the sizes of the train and test sets
print("Train set size:", len(data_train))
print("Test set size:", len(data_test))

# file_path = "train_data.json"
# data_train.to_json(file_path)

# file_path = "test_data.json"
# data_test.to_json(file_path)


file_path = "train_data.json"
data_train.reset_index().to_json(file_path,orient='records')


file_path = "test_data.json"
data_test.reset_index().to_json(file_path,orient='records')


###################################################

peft_model= '/home/ubuntu/Usman_Haider/falcon_aws/trained-model'

bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_use_double_quant = True,
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_compute_type = torch.bfloat16,
)

config = PeftConfig.from_pretrained(peft_model)
config.inference_mode = False

model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    quantization_config=bnb_config,
    device_map = "auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
tokenizer.pad_token = tokenizer.eos_token

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
	r=32,
	lora_alpha = 64,
	target_modules = ["query_key_value"],
	lora_dropout = 0.05,
	bias = 'none',
	task_type =  'CAUSAL_LM',
)
model = get_peft_model(model, peft_config)


###################################################
from datasets import load_dataset

data = load_dataset("json", data_files='train_data.json')
val_data = load_dataset("json", data_files='test_data.json')


def generate_prompt(quest):
  return f"""
    <human>:{quest["question"]}
    <assist>: {quest["answer"]}
    """.strip()

def generate_and_tokenize_prompt(data_point):
  full_prompt = generate_prompt(data_point)
  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)
  return tokenized_full_prompt

data = data['train'].shuffle().map(generate_and_tokenize_prompt)
val_data = val_data['train'].shuffle().map(generate_and_tokenize_prompt)

print(data)

# val_data


OUTPUT_DIR = "experiments"


# wandb login

import wandb
wandb.init(
    # set the wandb project where this run will be logged
    project="Falcon_7b_with_new_data",

    # track hyperparameters and run metadata
    config={
    "learning_rate": 2e-5,
    "architecture": "Falcon7b",
    "dataset": "Custom data with instance",
    "steps": 100,
    "r":32,
    "lora_alpha":64,

    }
)

training_args = transformers.TrainingArguments(
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 4,
    num_train_epochs = 10,
    learning_rate = 2e-5,
    fp16 = True,
    output_dir= OUTPUT_DIR,
    max_steps = 1000,
    optim = 'paged_adamw_8bit',
    lr_scheduler_type = "cosine",
    warmup_ratio = 0.05,
    logging_strategy="steps",
    logging_steps=1,
    evaluation_strategy="steps",
#    prediction_step = 5,
    save_strategy="steps",
    save_total_limit=2,
    load_best_model_at_end=True,
    report_to = 'wandb',
)


trainer = transformers.Trainer(
    model = model,
    args= training_args,
    train_dataset= data,
    eval_dataset = val_data,
    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm= False),
    # compute_metrics=compute_metrics,
)

model.config.use_cache = False
trainer.train()

model.save_pretrained('trained-model')

trained_model= 'trained-model'

# config = PeftConfig.from_pretrained(trained_model)


# model = AutoModelForCausalLM.from_pretrained(
#     config.base_model_name_or_path,
#     return_dict=True,
#     quantization_config=bnb_config,
#     device_map = "auto",
#     trust_remote_code=True,
# )

# model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training
# model_to_save.save_pretrained("outputs")
lora_config = LoraConfig.from_pretrained(trained_model)
model = get_peft_model(model, lora_config)

# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
# tokenizer.pad_token = tokenizer.eos_token

# model = PeftModel.from_pretrained(model, config)

# generation_config = model.generation_config
# generation_config.max_new_tokens = 300
# generation_config.temperature = 0.7
# generation_config.top_p = 0.7
# generation_config.num_return_sequences =1
# generation_config.pad_token_id = tokenizer.eos_token_id
# generation_config.eos_token_id = tokenizer.eos_token_id

device = "cuda:0"

def generate_response(question: str) -> str:

  prompt = f"""
  <human>: {question}
  <assist>:
  """.strip()
  print(prompt)

  encoding = tokenizer( prompt, return_tensors='pt').to(device)
  with torch.inference_mode():
    outputs = model.generate(
        input_ids = encoding.input_ids,
        attention_mask = encoding.attention_mask,
        generation_config = generation_config
    )

  resp = tokenizer.decode(outputs[0], skip_special=True)
  start = "<assist>"
  rep_start = resp.find(start)
  return resp[rep_start + len(start) :].strip()



quest = "What is Digital Processing Systems and what domain does it work in?"
print(generate_response(quest))

quest = "What was Khurram Nazir's role at PAF-Karachi Institute of Economics & Technology and National University of Computer and Emerging Sciences?"
print(generate_response(quest))

quest = "What is Anas Bin Arif's job title and where does he work?"
print(generate_response(quest))

quest = "Who is fahad jalal?"
print(generate_response(quest))

