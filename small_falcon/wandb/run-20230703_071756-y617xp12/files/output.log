
  0%|                                                                                                                             | 0/1000 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                   | 1/1000 [00:09<2:41:32,  9.70s/it]
{'loss': 4.9327, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.06}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.02s/it]

{'eval_loss': 4.554534912109375, 'eval_runtime': 6.7999, 'eval_samples_per_second': 7.206, 'eval_steps_per_second': 1.029, 'epoch': 0.06}
  0%|▏                                                                                                                  | 2/1000 [00:25<3:36:12, 13.00s/it]



 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:02<00:02,  1.23it/s]
{'eval_loss': 4.554498672485352, 'eval_runtime': 6.8564, 'eval_samples_per_second': 7.147, 'eval_steps_per_second': 1.021, 'epoch': 0.12}
  0%|▎                                                                                                                  | 3/1000 [00:38<3:40:08, 13.25s/it]



 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:02<00:02,  1.23it/s]
{'eval_loss': 4.5542426109313965, 'eval_runtime': 6.8735, 'eval_samples_per_second': 7.129, 'eval_steps_per_second': 1.018, 'epoch': 0.17}

  0%|▍                                                                                                                  | 4/1000 [00:52<3:46:55, 13.67s/it]


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.04s/it]

{'eval_loss': 4.554001331329346, 'eval_runtime': 6.9207, 'eval_samples_per_second': 7.08, 'eval_steps_per_second': 1.011, 'epoch': 0.23}
  0%|▌                                                                                                                  | 5/1000 [01:06<3:46:36, 13.66s/it]



 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:02<00:02,  1.22it/s]
{'eval_loss': 4.553509712219238, 'eval_runtime': 6.9297, 'eval_samples_per_second': 7.071, 'eval_steps_per_second': 1.01, 'epoch': 0.29}
  1%|▋                                                                                                                  | 6/1000 [01:20<3:45:24, 13.61s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:02,  1.00s/it]

{'eval_loss': 4.553168773651123, 'eval_runtime': 6.9809, 'eval_samples_per_second': 7.019, 'eval_steps_per_second': 1.003, 'epoch': 0.35}
  1%|▊                                                                                                                  | 7/1000 [01:34<3:47:58, 13.77s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:02,  1.01s/it]

{'eval_loss': 4.552145004272461, 'eval_runtime': 7.0046, 'eval_samples_per_second': 6.995, 'eval_steps_per_second': 0.999, 'epoch': 0.41}
  1%|▉                                                                                                                  | 8/1000 [01:48<3:51:05, 13.98s/it]



 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:02<00:02,  1.20it/s]
{'eval_loss': 4.551248073577881, 'eval_runtime': 7.0252, 'eval_samples_per_second': 6.975, 'eval_steps_per_second': 0.996, 'epoch': 0.46}
  1%|█                                                                                                                  | 9/1000 [02:01<3:48:05, 13.81s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:02,  1.01s/it]

{'eval_loss': 4.549965858459473, 'eval_runtime': 7.0544, 'eval_samples_per_second': 6.946, 'eval_steps_per_second': 0.992, 'epoch': 0.52}
  1%|█▏                                                                                                                | 10/1000 [02:16<3:49:21, 13.90s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:02,  1.01s/it]

{'eval_loss': 4.5484395027160645, 'eval_runtime': 7.059, 'eval_samples_per_second': 6.942, 'eval_steps_per_second': 0.992, 'epoch': 0.58}
  1%|█▎                                                                                                                | 11/1000 [02:30<3:51:58, 14.07s/it]



 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:02<00:02,  1.19it/s]
{'eval_loss': 4.546921253204346, 'eval_runtime': 7.0788, 'eval_samples_per_second': 6.922, 'eval_steps_per_second': 0.989, 'epoch': 0.64}
  1%|█▎                                                                                                                | 12/1000 [02:44<3:52:18, 14.11s/it]



 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.08s/it]

{'eval_loss': 4.544595718383789, 'eval_runtime': 7.1279, 'eval_samples_per_second': 6.874, 'eval_steps_per_second': 0.982, 'epoch': 0.7}
  1%|█▍                                                                                                                | 13/1000 [02:58<3:51:38, 14.08s/it]



 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.07s/it]

{'eval_loss': 4.542507648468018, 'eval_runtime': 7.1406, 'eval_samples_per_second': 6.862, 'eval_steps_per_second': 0.98, 'epoch': 0.75}

  1%|█▌                                                                                                                | 14/1000 [03:13<3:53:21, 14.20s/it]


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.08s/it]


  2%|█▋                                                                                                                | 15/1000 [03:27<3:53:54, 14.25s/it]
{'loss': 4.4513, 'learning_rate': 6e-06, 'epoch': 0.87}


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:02,  1.03s/it]

{'eval_loss': 4.536926746368408, 'eval_runtime': 7.1906, 'eval_samples_per_second': 6.814, 'eval_steps_per_second': 0.973, 'epoch': 0.87}
  2%|█▊                                                                                                                | 16/1000 [03:42<3:57:33, 14.48s/it]



 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:03<00:02,  1.17it/s]
{'eval_loss': 4.533350944519043, 'eval_runtime': 7.2059, 'eval_samples_per_second': 6.8, 'eval_steps_per_second': 0.971, 'epoch': 0.93}
  2%|█▉                                                                                                                | 17/1000 [03:56<3:55:27, 14.37s/it]



 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.09s/it]

{'eval_loss': 4.529691219329834, 'eval_runtime': 7.2104, 'eval_samples_per_second': 6.796, 'eval_steps_per_second': 0.971, 'epoch': 0.99}
  2%|██                                                                                                                | 18/1000 [04:10<3:54:07, 14.31s/it]



 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.09s/it]

  2%|██                                                                                                                | 18/1000 [04:18<3:54:07, 14.31s/it]Traceback (most recent call last):
  File "new_fine_falcon7b.py", line 237, in <module>
    trainer.train()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1850, in _inner_training_loop
    self.accelerator.clip_grad_norm_(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1893, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1856, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 275, in unscale_
    raise RuntimeError("unscale_() has already been called on this optimizer since the last update().")
RuntimeError: unscale_() has already been called on this optimizer since the last update().
Traceback (most recent call last):
  File "new_fine_falcon7b.py", line 237, in <module>
    trainer.train()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1850, in _inner_training_loop
    self.accelerator.clip_grad_norm_(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1893, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1856, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 275, in unscale_
    raise RuntimeError("unscale_() has already been called on this optimizer since the last update().")
RuntimeError: unscale_() has already been called on this optimizer since the last update().