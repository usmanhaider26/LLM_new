
  0%|                                                                                                                             | 0/1000 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                   | 1/1000 [00:10<3:01:34, 10.91s/it]
{'loss': 4.2974, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.06}


 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:03<00:02,  1.22it/s]
{'eval_loss': 4.558175086975098, 'eval_runtime': 6.5466, 'eval_samples_per_second': 7.485, 'eval_steps_per_second': 1.069, 'epoch': 0.06}
  0%|▏                                                                                                                  | 2/1000 [00:24<3:24:41, 12.31s/it]



 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:01,  1.10it/s]

  0%|▎                                                                                                                  | 3/1000 [00:37<3:30:42, 12.68s/it]
{'loss': 4.4923, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.17}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:00,  1.02it/s]

{'eval_loss': 4.558068752288818, 'eval_runtime': 6.637, 'eval_samples_per_second': 7.383, 'eval_steps_per_second': 1.055, 'epoch': 0.17}
  0%|▍                                                                                                                  | 4/1000 [00:50<3:35:47, 13.00s/it]



 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:03<00:02,  1.20it/s]
{'eval_loss': 4.557570934295654, 'eval_runtime': 6.6557, 'eval_samples_per_second': 7.362, 'eval_steps_per_second': 1.052, 'epoch': 0.23}
  0%|▌                                                                                                                  | 5/1000 [01:04<3:38:59, 13.21s/it]



 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:01,  1.08it/s]
  1%|▋                                                                                                                  | 6/1000 [01:18<3:44:52, 13.57s/it]
  0%|                                                                                                                                | 0/7 [00:00<?, ?it/s]



 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:01,  1.08it/s]
{'eval_loss': 4.556633949279785, 'eval_runtime': 6.7032, 'eval_samples_per_second': 7.31, 'eval_steps_per_second': 1.044, 'epoch': 0.35}
  1%|▊                                                                                                                  | 7/1000 [01:32<3:44:09, 13.54s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:01,  1.07it/s]

{'eval_loss': 4.555936813354492, 'eval_runtime': 6.7393, 'eval_samples_per_second': 7.271, 'eval_steps_per_second': 1.039, 'epoch': 0.41}
  1%|▉                                                                                                                  | 8/1000 [01:45<3:45:03, 13.61s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:04<00:01,  1.06it/s]


  1%|█                                                                                                                  | 9/1000 [01:59<3:44:43, 13.61s/it]
{'loss': 4.615, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.52}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.01s/it]


  1%|█▏                                                                                                                | 10/1000 [02:13<3:45:31, 13.67s/it]
{'loss': 4.2797, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.58}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.02s/it]

{'eval_loss': 4.5520124435424805, 'eval_runtime': 6.8501, 'eval_samples_per_second': 7.153, 'eval_steps_per_second': 1.022, 'epoch': 0.58}
  1%|█▎                                                                                                                | 11/1000 [02:27<3:45:36, 13.69s/it]



 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.03s/it]


  1%|█▎                                                                                                                | 12/1000 [02:41<3:48:54, 13.90s/it]
{'loss': 4.2998, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.7}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.03s/it]


  1%|█▍                                                                                                                | 13/1000 [02:55<3:47:48, 13.85s/it]
{'loss': 4.5506, 'learning_rate': 5.2e-06, 'epoch': 0.75}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.03s/it]


  1%|█▌                                                                                                                | 14/1000 [03:09<3:49:06, 13.94s/it]
{'loss': 4.3032, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.81}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.03s/it]


  2%|█▋                                                                                                                | 15/1000 [03:23<3:50:28, 14.04s/it]
{'loss': 4.5342, 'learning_rate': 6e-06, 'epoch': 0.87}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.03s/it]


  2%|█▊                                                                                                                | 16/1000 [03:37<3:49:04, 13.97s/it]
{'loss': 4.1587, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.93}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.04s/it]


  2%|█▉                                                                                                                | 17/1000 [03:51<3:49:46, 14.03s/it]
{'loss': 4.3492, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.99}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.04s/it]


  2%|██                                                                                                                | 18/1000 [04:05<3:48:56, 13.99s/it]
{'loss': 3.9872, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.04}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:01,  1.04s/it]


  2%|██▏                                                                                                               | 19/1000 [04:19<3:50:15, 14.08s/it]
{'loss': 4.5172, 'learning_rate': 7.600000000000001e-06, 'epoch': 1.1}
  File "new_fine_falcon7b.py", line 237, in <module>████▍                                                                    | 3/7 [00:02<00:03,  1.15it/s]
    trainer.train()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1696, in train
    return inner_training_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2052, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2352, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 3081, in evaluate
    output = eval_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 3262, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 3520, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2819, in compute_loss
    outputs = model(**inputs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/peft/peft_model.py", line 827, in forward
    return self.base_model(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 753, in forward
    transformer_outputs = self.transformer(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 648, in forward
    outputs = block(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 385, in forward
    attn_outputs = self.self_attention(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 245, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 205, in _split_heads
    return fused_qkv[..., :-2, :], fused_qkv[..., [-2], :], fused_qkv[..., [-1], :]
KeyboardInterrupt
Traceback (most recent call last):
  File "new_fine_falcon7b.py", line 237, in <module>
    trainer.train()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1696, in train
    return inner_training_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2052, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2352, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 3081, in evaluate
    output = eval_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 3262, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 3520, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2819, in compute_loss
    outputs = model(**inputs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/peft/peft_model.py", line 827, in forward
    return self.base_model(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 753, in forward
    transformer_outputs = self.transformer(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 648, in forward
    outputs = block(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 385, in forward
    attn_outputs = self.self_attention(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 245, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b-instruct/c7f670a03d987254220f343c6b026ea0c5147185/modelling_RW.py", line 205, in _split_heads
    return fused_qkv[..., :-2, :], fused_qkv[..., [-2], :], fused_qkv[..., [-1], :]
KeyboardInterrupt