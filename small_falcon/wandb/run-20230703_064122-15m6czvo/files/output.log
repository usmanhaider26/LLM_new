
  0%|                                                                                                                             | 0/1000 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                   | 1/1000 [00:11<3:04:38, 11.09s/it]
{'loss': 4.4064, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.06}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:04<00:00,  1.16it/s]

{'eval_loss': 4.55735969543457, 'eval_runtime': 6.5736, 'eval_samples_per_second': 7.454, 'eval_steps_per_second': 1.065, 'epoch': 0.06}
  0%|▏                                                                                                                  | 2/1000 [00:24<3:25:14, 12.34s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.27it/s]

{'eval_loss': 4.5572943687438965, 'eval_runtime': 6.6334, 'eval_samples_per_second': 7.387, 'eval_steps_per_second': 1.055, 'epoch': 0.12}
  0%|▎                                                                                                                  | 3/1000 [00:37<3:34:35, 12.91s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.27it/s]

{'eval_loss': 4.5570478439331055, 'eval_runtime': 6.6497, 'eval_samples_per_second': 7.369, 'eval_steps_per_second': 1.053, 'epoch': 0.17}
  0%|▍                                                                                                                  | 4/1000 [00:51<3:39:48, 13.24s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.25it/s]


  0%|▌                                                                                                                  | 5/1000 [01:05<3:42:50, 13.44s/it]
{'loss': 4.295, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.29}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:04<00:00,  1.12it/s]


  1%|▋                                                                                                                  | 6/1000 [01:19<3:44:51, 13.57s/it]
{'loss': 4.4462, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.35}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:04<00:00,  1.11it/s]

{'eval_loss': 4.5559306144714355, 'eval_runtime': 6.8004, 'eval_samples_per_second': 7.205, 'eval_steps_per_second': 1.029, 'epoch': 0.35}

  1%|▊                                                                                                                  | 7/1000 [01:32<3:45:05, 13.60s/it]


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:04<00:00,  1.10it/s]


  1%|▉                                                                                                                  | 8/1000 [01:47<3:49:39, 13.89s/it]
{'loss': 4.5782, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.46}


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:04<00:00,  1.10it/s]

{'eval_loss': 4.553897380828857, 'eval_runtime': 6.8324, 'eval_samples_per_second': 7.172, 'eval_steps_per_second': 1.025, 'epoch': 0.46}
  1%|█                                                                                                                  | 9/1000 [02:01<3:52:35, 14.08s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.22it/s]

{'eval_loss': 4.552732944488525, 'eval_runtime': 6.8702, 'eval_samples_per_second': 7.132, 'eval_steps_per_second': 1.019, 'epoch': 0.52}
  1%|█▏                                                                                                                | 10/1000 [02:16<3:52:16, 14.08s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.22it/s]

{'eval_loss': 4.5513153076171875, 'eval_runtime': 6.9018, 'eval_samples_per_second': 7.1, 'eval_steps_per_second': 1.014, 'epoch': 0.58}

  1%|█▎                                                                                                                | 11/1000 [02:30<3:54:01, 14.20s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.21it/s]
{'eval_loss': 4.549448013305664, 'eval_runtime': 6.9455, 'eval_samples_per_second': 7.055, 'eval_steps_per_second': 1.008, 'epoch': 0.64}

  1%|█▎                                                                                                                | 12/1000 [02:44<3:53:31, 14.18s/it]


 57%|████████████████████████████████████████████████████████████████████▌                                                   | 4/7 [00:03<00:02,  1.23it/s]
{'eval_loss': 4.547676086425781, 'eval_runtime': 6.9639, 'eval_samples_per_second': 7.036, 'eval_steps_per_second': 1.005, 'epoch': 0.7}

  1%|█▍                                                                                                                | 13/1000 [02:58<3:53:48, 14.21s/it]


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:00,  1.08it/s]

{'eval_loss': 4.545130729675293, 'eval_runtime': 6.9897, 'eval_samples_per_second': 7.01, 'eval_steps_per_second': 1.001, 'epoch': 0.75}
  1%|█▌                                                                                                                | 14/1000 [03:13<3:57:29, 14.45s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.20it/s]


  2%|█▋                                                                                                                | 15/1000 [03:27<3:53:34, 14.23s/it]
{'loss': 4.4278, 'learning_rate': 6e-06, 'epoch': 0.87}


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.20it/s]

{'eval_loss': 4.5392045974731445, 'eval_runtime': 7.038, 'eval_samples_per_second': 6.962, 'eval_steps_per_second': 0.995, 'epoch': 0.87}
  2%|█▊                                                                                                                | 16/1000 [03:42<3:54:54, 14.32s/it]


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.19it/s]


  2%|█▉                                                                                                                | 17/1000 [03:55<3:51:17, 14.12s/it]
{'loss': 4.9149, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.99}


 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 5/7 [00:03<00:01,  1.20it/s]

{'eval_loss': 4.5321831703186035, 'eval_runtime': 7.0566, 'eval_samples_per_second': 6.944, 'eval_steps_per_second': 0.992, 'epoch': 0.99}

  2%|██                                                                                                                | 18/1000 [04:09<3:47:18, 13.89s/it]


 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 6/7 [00:05<00:00,  1.07it/s]

  2%|██                                                                                                                | 18/1000 [04:16<3:47:18, 13.89s/it]Traceback (most recent call last):
  File "new_fine_falcon7b.py", line 237, in <module>
    trainer.train()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1850, in _inner_training_loop
    self.accelerator.clip_grad_norm_(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1893, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1856, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 275, in unscale_
    raise RuntimeError("unscale_() has already been called on this optimizer since the last update().")
RuntimeError: unscale_() has already been called on this optimizer since the last update().
Traceback (most recent call last):
  File "new_fine_falcon7b.py", line 237, in <module>
    trainer.train()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1850, in _inner_training_loop
    self.accelerator.clip_grad_norm_(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1893, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/accelerate/accelerator.py", line 1856, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 275, in unscale_
    raise RuntimeError("unscale_() has already been called on this optimizer since the last update().")
RuntimeError: unscale_() has already been called on this optimizer since the last update().