# -*- coding: utf-8 -*-
"""Lateste_Falcon7b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Mqp6Z3M7qbxytKCp0hggL1Vg7SNSK4y
"""

# from google.colab import drive
# drive.mount('drive')

# !nvidia-smi

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

# !pip install -q -U bitsandbytes
# !pip install -q -U git+https://github.com/huggingface/transformers.git
# !pip install -q -U git+https://github.com/huggingface/peft.git
# !pip install -q -U git+https://github.com/huggingface/accelerate.git
# !pip install -q -U einops
# !pip install -q -U safetensors
# !pip install -q -U torch
# !pip install -q -U xformers
# !pip install -q -U datasets

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
import transformers
import torch
from torch.utils.data import DataLoader, Dataset
import torch
from transformers import AutoTokenizer
import os
import wandb
from peft import (
    LoraConfig,
    PeftConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)

# !unzip Generated_data.zip

path = "Generated_data_2x"
dirs = os.listdir( path )

import pandas as pd
import json
data= []
id = 1
for files in dirs:
  f = open(os.path.join(path, files))
  j_data = json.load(f)
  for dd in j_data:
    if all(key in dd for key in ["question", "answer"]):
      temp = {'question': dd['question'],'answer':dd['answer']}
      data.append(temp)
    if all(key in dd for key in ["Question", "Answer"]):
      temp = {'question': dd['Question'],'answer':dd['Answer']}
      data.append(temp)
  # Closing file
  f.close()

df_data = pd.DataFrame(data)

df_data

import re

# Preprocess question column
df_data['question'] = df_data['question'].apply(lambda x: re.sub(r'[^\w\s]', '', x))  # Remove special characters
df_data['question'] = df_data['question'].apply(lambda x: x.lower())  # Convert to lowercase

# Preprocess answer column
df_data['answer'] = df_data['answer'].apply(lambda x: re.sub(r'[^\w\s]', '', x))  # Remove special characters
df_data['answer'] = df_data['answer'].apply(lambda x: x.lower())  # Convert to lowercase

# Display the preprocessed dataframe
print(df_data.head())

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

shuffled_df = shuffle(df_data, random_state=42)

# Split the shuffled DataFrame into train and test sets
data_train, data_test = train_test_split(shuffled_df, test_size=0.15, random_state=42)

# Print the sizes of the train and test sets
print("Train set size:", len(data_train))
print("Test set size:", len(data_test))

# file_path = "train_data.json"
# data_train.to_json(file_path)

# file_path = "test_data.json"
# data_test.to_json(file_path)


file_path = "train_data.json"
data_train.reset_index().to_json(file_path,orient='records')


file_path = "test_data.json"
data_test.reset_index().to_json(file_path,orient='records')

model_id = "vilsonrodrigues/falcon-7b-instruct-sharded" # sharded model by vilsonrodrigues
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0}, trust_remote_code=True)

def print_trainable_paramters(model):

  trainable_parms = 0
  all_parms = 0

  for _, param in model.name_parameters():
    all_parms += param.numel()
    if param.requires_grad:
      trainable_parms += param.numel()
  print(
      f'Trainable parameters: {trainable_parms} || all parms: {all_parms} || trainable: {100*trainable_parms / all_parms}'
  )

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

config  = LoraConfig(
    r = 32,
    lora_alpha = 64,
    target_modules = ["q_proj", "v_proj","query_key_value"],
    lora_dropout = 0.05,
    bias = 'none',
    task_type = 'CAUSAL_LM',
)

model = get_peft_model(model, config)
# print_trainable_paramters(model)

# prompt = f"""
# <human>: Who is fahad jalal?
# <assist>:
# """.strip()
# print(prompt)

# generation_config = model.generation_config
# generation_config.max_new_tokens = 200
# generation_config.temperature = 0.7
# generation_config.top_p = 0.7
# generation_config.num_return_sequences =1
# generation_config.pad_token_id = tokenizer.eos_token_id
# generation_config.eos_token_id = tokenizer.eos_token_id

# device = "cuda:0"

# encoding = tokenizer( prompt, return_tensors='pt').to(device)
# with torch.inference_mode():
#   outputs = model.generate(
#       input_ids = encoding.input_ids,
#       attention_mask = encoding.attention_mask,
#       generation_config = generation_config
#   )

# print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from datasets import load_dataset

data = load_dataset("json", data_files='train_data.json')
val_data = load_dataset("json", data_files='test_data.json')

data

val_data

def generate_prompt(quest):
  return f"""
    <human>:{quest["question"]}
    <assist>: {quest["answer"]}
    """.strip()

def generate_and_tokenize_prompt(data_point):
  full_prompt = generate_prompt(data_point)
  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)
  return tokenized_full_prompt

data = data['train'].shuffle().map(generate_and_tokenize_prompt)
val_data = val_data['train'].shuffle().map(generate_and_tokenize_prompt)

data

val_data

OUTPUT_DIR = "experiments"

# %load_ext tensorboard
# %tensorboard --logdir experiments/runs

# !pip install evaluate
# !pip install rouge_score

# import evaluate
# import nltk
# import numpy as np
# from nltk.tokenize import sent_tokenize
# nltk.download("punkt")

# # Metric
# metric = evaluate.load("rouge")

# # helper function to postprocess text
# def postprocess_text(preds, labels):
#     preds = [pred.strip() for pred in preds]
#     labels = [label.strip() for label in labels]

#     # rougeLSum expects newline after each sentence
#     preds = ["\n".join(sent_tokenize(pred)) for pred in preds]
#     labels = ["\n".join(sent_tokenize(label)) for label in labels]

#     return preds, labels

# def compute_metrics(eval_preds):
#     preds, labels = eval_preds
#     if isinstance(preds, tuple):
#         preds = preds[0]
#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
#     # Replace -100 in the labels as we can't decode them.
#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

#     # Some simple post-processing
#     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

#     result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
#     result = {k: round(v * 100, 4) for k, v in result.items()}
#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
#     result["gen_len"] = np.mean(prediction_lens)
#     return result

#wandb login

import wandb
wandb.init(
    # set the wandb project where this run will be logged
    project="Falcon_7b_with_val_aws",

    # track hyperparameters and run metadata
    config={
    "learning_rate": 2e-5,
    "architecture": "Falcon7b",
    "dataset": "Custom data with instance",
    "epochs": 80,
    "r":128,
    "lora_alpha":256,

    }
)

training_args = transformers.TrainingArguments(
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 4,
    num_train_epochs = 10,
    learning_rate = 2e-5,
    fp16 = True,
    output_dir= OUTPUT_DIR,
    max_steps = 1000,
    optim = 'paged_adamw_8bit',
    lr_scheduler_type = "cosine",
    warmup_ratio = 0.05,
    logging_strategy="steps",
    logging_steps=1,
    evaluation_strategy="steps",
    save_strategy="steps",
    # save_total_limit=2,
    load_best_model_at_end=True,
    report_to = 'wandb',
)


trainer = transformers.Trainer(
    model = model,
    args= training_args,
    train_dataset= data,
    eval_dataset = val_data,
    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm= False),
    # compute_metrics=compute_metrics,
)

model.config.use_cache = False
trainer.train()

model.save_pretrained('trained-model')

trained_model= 'trained-model'

# config = PeftConfig.from_pretrained(trained_model)


# model = AutoModelForCausalLM.from_pretrained(
#     config.base_model_name_or_path,
#     return_dict=True,
#     quantization_config=bnb_config,
#     device_map = "auto",
#     trust_remote_code=True,
# )

# model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training
# model_to_save.save_pretrained("outputs")
lora_config = LoraConfig.from_pretrained(trained_model)
model = get_peft_model(model, lora_config)

# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
# tokenizer.pad_token = tokenizer.eos_token

# model = PeftModel.from_pretrained(model, config)

# generation_config = model.generation_config
# generation_config.max_new_tokens = 300
# generation_config.temperature = 0.7
# generation_config.top_p = 0.7
# generation_config.num_return_sequences =1
# generation_config.pad_token_id = tokenizer.eos_token_id
# generation_config.eos_token_id = tokenizer.eos_token_id

device = "cuda:0"

def generate_response(question: str) -> str:

  prompt = f"""
  <human>: {question}
  <assist>:
  """.strip()
  print(prompt)

  encoding = tokenizer( prompt, return_tensors='pt').to(device)
  with torch.inference_mode():
    outputs = model.generate(
        input_ids = encoding.input_ids,
        attention_mask = encoding.attention_mask,
        generation_config = generation_config
    )

  resp = tokenizer.decode(outputs[0], skip_special=True)
  start = "<assist>"
  rep_start = resp.find(start)
  return resp[rep_start + len(start) :].strip()



quest = "What is Digital Processing Systems and what domain does it work in?"
print(generate_response(quest))

quest = "What was Khurram Nazir's role at PAF-Karachi Institute of Economics & Technology and National University of Computer and Emerging Sciences?"
print(generate_response(quest))

quest = "What is Anas Bin Arif's job title and where does he work?"
print(generate_response(quest))

quest = "Who is fahad jalal?"
print(generate_response(quest))

